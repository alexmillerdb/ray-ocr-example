{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcab6e0-cd8e-4499-9cdb-3d1ac9d7ff7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'alex.miller-aeJwaPsN (Python 3.11.7)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/alex.miller/.local/share/virtualenvs/alex.miller-aeJwaPsN/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install pytesseract pdf2image\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710e70ec-1c25-4fad-a6bd-9348d46e4d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster Config to use for reproducibility:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"cluster_name\": \"alex miller OCR cluster udf\",\n",
    "    \"spark_version\": \"15.4.x-scala2.12\",\n",
    "    \"spark_conf\": {\n",
    "        \"spark.databricks.pyspark.dataFrameChunk.enabled\": \"true\"\n",
    "    },\n",
    "    \"azure_attributes\": {\n",
    "        \"availability\": \"ON_DEMAND_AZURE\"\n",
    "    },\n",
    "    \"node_type_id\": \"Standard_D16ads_v5\",\n",
    "    \"autotermination_minutes\": 120,\n",
    "    \"init_scripts\": [\n",
    "        {\n",
    "            \"workspace\": {\n",
    "                \"destination\": \"/Users/alex.miller@databricks.com/ray-ocr/init.sh\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"single_user_name\": \"alex.miller@databricks.com\",\n",
    "    \"data_security_mode\": \"DATA_SECURITY_MODE_AUTO\",\n",
    "    \"runtime_engine\": \"PHOTON\",\n",
    "    \"kind\": \"CLASSIC_PREVIEW\",\n",
    "    \"use_ml_runtime\": true,\n",
    "    \"is_single_node\": false,\n",
    "    \"num_workers\": 6,\n",
    "    \"apply_policy_default_values\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937320f1-f210-493b-9f9b-f76ee1e99792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Ray cluster:\n",
    "- init.sh is included in Databricks cluster to download tesseract-ocr package to all nodes\n",
    "- Spark Cluster has 6 `num_workers` but will pass 4 to Ray and leave 2 for Spark (let Spark handle to read and write process)\n",
    "- Supplying Ray with 4 `min_worker_nodes` and `max_worker_nodes` (leaving 2 for Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b030301-5208-4ea3-83ed-6322df107be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.spark import setup_ray_cluster, MAX_NUM_WORKER_NODES, shutdown_ray_cluster\n",
    "import ray\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Ray allows you to define custom cluster configurations using setup_ray_cluster function\n",
    "# This allows you to allocate CPUs and GPUs on Ray cluster\n",
    "ray_context = setup_ray_cluster(\n",
    "  min_worker_nodes=4,       # minimum number of worker nodes to start\n",
    "  max_worker_nodes=4,       # maximum number of worker nodes to start (autoscaling)\n",
    "  num_gpus_worker_node=0,   # number of GPUs to allocate per worker node\n",
    "  num_gpus_head_node=0,     # number of GPUs to allocate on head node (driver)\n",
    "  num_cpus_worker_node=12,   # number of CPUs to allocate on worker nodes, only giving Ray 1 and Spark the rest\n",
    "  num_cpus_head_node=8,    # number of CPUs to allocate on head node (driver)\n",
    "  collect_log_tp_path=\"/Volumes/alex_m/gen_ai/pdfs/ray_collected_logs\"\n",
    ")\n",
    "\n",
    "# Pass any custom configuration to ray.init\n",
    "ray.init(ignore_reinit_error=True)\n",
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "### Description of the Code\n",
    "\n",
    "The code below is designed to process PDF documents using a combination of Spark and Ray for distributed computing. The workflow involves the following steps:\n",
    "\n",
    "1. **Cluster Setup**: The Ray cluster is set up with specific configurations for the number of worker nodes, CPUs, and GPUs allocated to both the head node and worker nodes. This setup ensures efficient resource utilization between Spark and Ray.\n",
    "\n",
    "2. **PDF Processing**: A `PDFProcessor` class is defined to convert PDF documents into images. The class includes methods to convert PDF data to image bytes and handle batches of PDF documents.\n",
    "\n",
    "3. **OCR Processing**: An `OCRProcessor` class is defined to perform Optical Character Recognition (OCR) on the images generated from the PDF documents. The class includes methods to convert image bytes to PIL images and handle batches of images for OCR processing.\n",
    "\n",
    "4. **Main Function**: The `main` function orchestrates the entire workflow:\n",
    "    - Reads PDF documents from a Spark table.\n",
    "    - Converts the Spark DataFrame to a Ray Dataset.\n",
    "    - Processes the PDFs to convert them into images using the `PDFProcessor`.\n",
    "    - Performs OCR on the images using the `OCRProcessor`.\n",
    "    - Converts the OCR results to a Pandas DataFrame and displays the results.\n",
    "    - Saves the processed data back to a Spark table.\n",
    "\n",
    "5. **Shutdown**: Finally, the Ray cluster is shut down to release the resources.\n",
    "\n",
    "This approach leverages the parallel processing capabilities of Ray and the distributed data handling capabilities of Spark to efficiently process large volumes of PDF documents and extract text using OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f31f4c-3f3a-4760-9221-065a9992f415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List, Dict, Any\n",
    "import ray\n",
    "from pdf2image import convert_from_bytes\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing PDF documents and converting them to images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def pdf_to_image_bytes(pdf_data: bytes) -> List[bytes]:\n",
    "        \"\"\"\n",
    "        Convert PDF data to a list of image byte strings.\n",
    "\n",
    "        Args:\n",
    "            pdf_data (bytes): Raw PDF data.\n",
    "\n",
    "        Returns:\n",
    "            List[bytes]: List of image byte strings.\n",
    "        \"\"\"\n",
    "        pages = convert_from_bytes(pdf_data)\n",
    "        return [PDFProcessor._image_to_bytes(page) for page in pages]\n",
    "\n",
    "    @staticmethod\n",
    "    def _image_to_bytes(image: Image.Image) -> bytes:\n",
    "        \"\"\"\n",
    "        Convert a PIL Image to bytes.\n",
    "\n",
    "        Args:\n",
    "            image (Image.Image): PIL Image object.\n",
    "\n",
    "        Returns:\n",
    "            bytes: Byte string representation of the image.\n",
    "        \"\"\"\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        return img_byte_arr.getvalue()\n",
    "\n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Process a batch of PDF documents.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, Any]): Batch of PDF documents.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Any]]: Processed batch with pages, paths, and page numbers.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for content, path in zip(batch[\"content\"], batch[\"path\"]):\n",
    "            try:\n",
    "                pages = PDFProcessor.pdf_to_image_bytes(content)\n",
    "                results.extend([\n",
    "                    {\"page\": page, \"path\": path, \"page_number\": i + 1}\n",
    "                    for i, page in enumerate(pages)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"page\": b\"\",\n",
    "                    \"path\": path,\n",
    "                    \"page_number\": -1,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        return {\n",
    "            \"page\": [item[\"page\"] for item in results],\n",
    "            \"path\": [item[\"path\"] for item in results],\n",
    "            \"page_number\": [item[\"page_number\"] for item in results],\n",
    "            \"error\": [item.get(\"error\", \"\") for item in results]\n",
    "        }\n",
    "\n",
    "class OCRProcessor:\n",
    "    \"\"\"\n",
    "    A class for performing OCR on images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tesseract = pytesseract\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_pil(image_bytes: bytes) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Convert image bytes to PIL Image.\n",
    "\n",
    "        Args:\n",
    "            image_bytes (bytes): Byte string representation of an image.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: PIL Image object.\n",
    "        \"\"\"\n",
    "        return Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # def process_batch(self, batch: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Process a batch of images with OCR.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, Any]): Batch of images.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Any]]: OCR results for the batch.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for page, path, page_number in zip(batch[\"page\"], batch[\"path\"], batch[\"page_number\"]):\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                image = self.bytes_to_pil(page)\n",
    "                text = self.tesseract.image_to_string(image)\n",
    "                results.append({\n",
    "                    \"text\": text or \"\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"error\": \"\",\n",
    "                    \"path\": path,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"duration\": time.time() - start_time\n",
    "                })\n",
    "                del image\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"text\": \"\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                    \"path\": path,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"duration\": time.time() - start_time\n",
    "                })\n",
    "\n",
    "        return {key: [item[key] for item in results] for key in results[0]}\n",
    "\n",
    "# def main():\n",
    "#     # Define Spark DataFrame column names for PDFs\n",
    "#     pdf_column = \"pdf\"\n",
    "#     path_column = \"__url__\"\n",
    "\n",
    "#     # Read PDFs from Spark table\n",
    "#     sdf = (\n",
    "#         spark.read.table(\"alex_m.gen_ai.pixparse_pdfs\")\n",
    "#         .select(F.col(pdf_column).alias(\"content\"), F.col(path_column).alias(\"path\"))\n",
    "#         .limit(1000)\n",
    "#     )\n",
    "\n",
    "#     # Create Ray Dataset\n",
    "#     ray_dataset = ray.data.from_spark(sdf)\n",
    "\n",
    "#     # Set concurrency\n",
    "#     min_concurrency = int(ray.cluster_resources().get(\"CPU\", 1) * 0.4)\n",
    "#     max_concurrency = int(ray.cluster_resources().get(\"CPU\", 1))\n",
    "#     max_concurrency = int(ray.cluster_resources().get(\"CPU\", 1) * 0.9)\n",
    "\n",
    "#     # Process PDFs\n",
    "#     pages_dataset = ray_dataset.map_batches(\n",
    "#         PDFProcessor,\n",
    "#         # PDFProcessor.process_batch,\n",
    "#         batch_size=100,\n",
    "#         num_cpus=1,\n",
    "#         # concurrency=max_concurrency\n",
    "#         concurrency=(15, 28),\n",
    "#     )\n",
    "\n",
    "#     # Perform OCR\n",
    "#     # ocr_processor = OCRProcessor()\n",
    "#     # ocr_processor = OCRProcessor.remote()\n",
    "\n",
    "#     ocr_dataset = pages_dataset.map_batches(\n",
    "#         OCRProcessor,\n",
    "#         # ocr_processor.process_batch,\n",
    "#         batch_size=8,\n",
    "#         num_cpus=1,\n",
    "#         concurrency=(20, 28),   # recommendation from Ray docs?\n",
    "#     )\n",
    "\n",
    "#     # Convert to pandas and display results\n",
    "#     ocr_dataset_pd = ocr_dataset.to_pandas()\n",
    "#     ocr_dataset_pd.display()\n",
    "\n",
    "#     processed_spark_df = spark.createDataFrame(ocr_dataset_pd)\n",
    "#     processed_spark_df.write.mode(\"overwrite\").saveAsTable(\"alex_m.gen_ai.ray_ocr\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "def main(config: ProcessingConfig = ProcessingConfig()) -> None:\n",
    "    \"\"\"\n",
    "    Main processing pipeline for PDF OCR.\n",
    "    \n",
    "    Args:\n",
    "        config: ProcessingConfig object containing all processing parameters\n",
    "    \"\"\"\n",
    "    # Read PDFs from Spark table\n",
    "    sdf = (\n",
    "        spark.read.table(config.data.input_table)\n",
    "        .select(\n",
    "            F.col(config.data.pdf_column).alias(\"content\"), \n",
    "            F.col(config.data.path_column).alias(\"path\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if config.data.limit_rows:\n",
    "        sdf = sdf.limit(config.data.limit_rows)\n",
    "\n",
    "    # Create Ray Dataset\n",
    "    ray_dataset = ray.data.from_spark(sdf)\n",
    "\n",
    "    # Set concurrency based on available CPUs\n",
    "    cpu_count = ray.cluster_resources().get(\"CPU\", 1)\n",
    "    min_concurrency = int(cpu_count * config.ray.min_concurrency_factor)\n",
    "    max_concurrency = int(cpu_count * config.ray.max_concurrency_factor)\n",
    "\n",
    "    # Process PDFs\n",
    "    pages_dataset = ray_dataset.map_batches(\n",
    "        PDFProcessor,\n",
    "        batch_size=config.ray.pdf_batch_size,\n",
    "        num_cpus=config.ray.pdf_num_cpus,\n",
    "        concurrency=config.ray.pdf_concurrency,\n",
    "    )\n",
    "\n",
    "    # Perform OCR\n",
    "    ocr_dataset = pages_dataset.map_batches(\n",
    "        OCRProcessor,\n",
    "        batch_size=config.ray.ocr_batch_size,\n",
    "        num_cpus=config.ray.ocr_num_cpus,\n",
    "        concurrency=config.ray.ocr_concurrency,\n",
    "    )\n",
    "\n",
    "    # Convert to pandas and display results\n",
    "    ocr_dataset_pd = ocr_dataset.to_pandas()\n",
    "    ocr_dataset_pd.display()\n",
    "\n",
    "    # Save results\n",
    "    processed_spark_df = spark.createDataFrame(ocr_dataset_pd)\n",
    "    processed_spark_df.write.mode(\"overwrite\").saveAsTable(config.data.output_table)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import ProcessingConfig, RayConfig, DataConfig\n",
    "    # Example usage with custom configuration\n",
    "    config = ProcessingConfig(\n",
    "        ray=RayConfig(\n",
    "            pdf_batch_size=100,\n",
    "            pdf_num_cpus=1,\n",
    "            pdf_concurrency=(20, 28),\n",
    "            ocr_batch_size=8,\n",
    "            ocr_num_cpus=1,\n",
    "            ocr_concurrency=(20, 28)\n",
    "        ),\n",
    "        data=DataConfig(\n",
    "            limit_rows=1000,\n",
    "            input_table=\"alex_m.gen_ai.pixparse_pdfs\",\n",
    "            pdf_column=\"pdf\",\n",
    "            path_column=\"__url__\",\n",
    "            output_table=\"alex_m.gen_ai.ray_ocr\"\n",
    "        )\n",
    "    )\n",
    "    main(config)\n",
    "    \n",
    "    shutdown_ray_cluster()\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-ray-ocr-pipeline-spark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "alex.miller-aeJwaPsN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
