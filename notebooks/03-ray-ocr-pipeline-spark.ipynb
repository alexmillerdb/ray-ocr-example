{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcab6e0-cd8e-4499-9cdb-3d1ac9d7ff7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytesseract pdf2image\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710e70ec-1c25-4fad-a6bd-9348d46e4d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster Config to use for reproducibility:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"cluster_name\": \"alex miller OCR cluster udf\",\n",
    "    \"spark_version\": \"15.4.x-scala2.12\",\n",
    "    \"spark_conf\": {\n",
    "        \"spark.databricks.pyspark.dataFrameChunk.enabled\": \"true\"\n",
    "    },\n",
    "    \"azure_attributes\": {\n",
    "        \"availability\": \"ON_DEMAND_AZURE\"\n",
    "    },\n",
    "    \"node_type_id\": \"Standard_D16ads_v5\",\n",
    "    \"autotermination_minutes\": 120,\n",
    "    \"init_scripts\": [\n",
    "        {\n",
    "            \"workspace\": {\n",
    "                \"destination\": \"/Users/alex.miller@databricks.com/ray-ocr/init.sh\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"single_user_name\": \"alex.miller@databricks.com\",\n",
    "    \"data_security_mode\": \"DATA_SECURITY_MODE_AUTO\",\n",
    "    \"runtime_engine\": \"PHOTON\",\n",
    "    \"kind\": \"CLASSIC_PREVIEW\",\n",
    "    \"use_ml_runtime\": true,\n",
    "    \"is_single_node\": false,\n",
    "    \"num_workers\": 6,\n",
    "    \"apply_policy_default_values\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937320f1-f210-493b-9f9b-f76ee1e99792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Ray cluster:\n",
    "- init.sh is included in Databricks cluster to download tesseract-ocr package to all nodes\n",
    "- Spark Cluster has 6 `num_workers` but will pass 4 to Ray and leave 2 for Spark (let Spark handle to read and write process)\n",
    "- Supplying Ray with 4 `min_worker_nodes` and `max_worker_nodes` (leaving 2 for Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b030301-5208-4ea3-83ed-6322df107be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.spark import setup_ray_cluster, MAX_NUM_WORKER_NODES, shutdown_ray_cluster\n",
    "import ray\n",
    "\n",
    "restart = True\n",
    "if restart is True:\n",
    "  try:\n",
    "    shutdown_ray_cluster()\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    ray.shutdown()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Ray allows you to define custom cluster configurations using setup_ray_cluster function\n",
    "# This allows you to allocate CPUs and GPUs on Ray cluster\n",
    "ray_context = setup_ray_cluster(\n",
    "  min_worker_nodes=4,       # minimum number of worker nodes to start\n",
    "  max_worker_nodes=4,       # maximum number of worker nodes to start (autoscaling)\n",
    "  num_gpus_worker_node=0,   # number of GPUs to allocate per worker node\n",
    "  num_gpus_head_node=0,     # number of GPUs to allocate on head node (driver)\n",
    "  num_cpus_worker_node=12,   # number of CPUs to allocate on worker nodes, only giving Ray 1 and Spark the rest\n",
    "  num_cpus_head_node=8,    # number of CPUs to allocate on head node (driver)\n",
    "  collect_log_tp_path=\"/Volumes/alex_m/gen_ai/pdfs/ray_collected_logs\"\n",
    ")\n",
    "\n",
    "# Pass any custom configuration to ray.init\n",
    "ray.init(ignore_reinit_error=True)\n",
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba50916c-06e2-43ca-8b0a-820f50687385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### THIS!\n",
    "\n",
    "- Need to test adding pools of actors to code to see if it improves processing time\n",
    "- Test different batch_sizes, num_cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f31f4c-3f3a-4760-9221-065a9992f415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List, Dict, Any\n",
    "import ray\n",
    "from pdf2image import convert_from_bytes\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing PDF documents and converting them to images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def pdf_to_image_bytes(pdf_data: bytes) -> List[bytes]:\n",
    "        \"\"\"\n",
    "        Convert PDF data to a list of image byte strings.\n",
    "\n",
    "        Args:\n",
    "            pdf_data (bytes): Raw PDF data.\n",
    "\n",
    "        Returns:\n",
    "            List[bytes]: List of image byte strings.\n",
    "        \"\"\"\n",
    "        pages = convert_from_bytes(pdf_data)\n",
    "        return [PDFProcessor._image_to_bytes(page) for page in pages]\n",
    "\n",
    "    @staticmethod\n",
    "    def _image_to_bytes(image: Image.Image) -> bytes:\n",
    "        \"\"\"\n",
    "        Convert a PIL Image to bytes.\n",
    "\n",
    "        Args:\n",
    "            image (Image.Image): PIL Image object.\n",
    "\n",
    "        Returns:\n",
    "            bytes: Byte string representation of the image.\n",
    "        \"\"\"\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        return img_byte_arr.getvalue()\n",
    "\n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Process a batch of PDF documents.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, Any]): Batch of PDF documents.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Any]]: Processed batch with pages, paths, and page numbers.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for content, path in zip(batch[\"content\"], batch[\"path\"]):\n",
    "            try:\n",
    "                pages = PDFProcessor.pdf_to_image_bytes(content)\n",
    "                results.extend([\n",
    "                    {\"page\": page, \"path\": path, \"page_number\": i + 1}\n",
    "                    for i, page in enumerate(pages)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"page\": b\"\",\n",
    "                    \"path\": path,\n",
    "                    \"page_number\": -1,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        return {\n",
    "            \"page\": [item[\"page\"] for item in results],\n",
    "            \"path\": [item[\"path\"] for item in results],\n",
    "            \"page_number\": [item[\"page_number\"] for item in results],\n",
    "            \"error\": [item.get(\"error\", \"\") for item in results]\n",
    "        }\n",
    "\n",
    "class OCRProcessor:\n",
    "    \"\"\"\n",
    "    A class for performing OCR on images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tesseract = pytesseract\n",
    "\n",
    "    @staticmethod\n",
    "    def bytes_to_pil(image_bytes: bytes) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Convert image bytes to PIL Image.\n",
    "\n",
    "        Args:\n",
    "            image_bytes (bytes): Byte string representation of an image.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: PIL Image object.\n",
    "        \"\"\"\n",
    "        return Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # def process_batch(self, batch: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "    def __call__(self, batch: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Process a batch of images with OCR.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, Any]): Batch of images.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Any]]: OCR results for the batch.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for page, path, page_number in zip(batch[\"page\"], batch[\"path\"], batch[\"page_number\"]):\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                image = self.bytes_to_pil(page)\n",
    "                text = self.tesseract.image_to_string(image)\n",
    "                results.append({\n",
    "                    \"text\": text or \"\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"error\": \"\",\n",
    "                    \"path\": path,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"duration\": time.time() - start_time\n",
    "                })\n",
    "                del image\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"text\": \"\",\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                    \"path\": path,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"duration\": time.time() - start_time\n",
    "                })\n",
    "\n",
    "        return {key: [item[key] for item in results] for key in results[0]}\n",
    "\n",
    "def main():\n",
    "    # Define Spark DataFrame column names for PDFs\n",
    "    pdf_column = \"pdf\"\n",
    "    path_column = \"__url__\"\n",
    "\n",
    "    # Read PDFs from Spark table\n",
    "    sdf = (\n",
    "        spark.read.table(\"alex_m.gen_ai.pixparse_pdfs\")\n",
    "        .select(F.col(pdf_column).alias(\"content\"), F.col(path_column).alias(\"path\"))\n",
    "        .limit(1000)\n",
    "    )\n",
    "\n",
    "    # Create Ray Dataset\n",
    "    ray_dataset = ray.data.from_spark(sdf)\n",
    "\n",
    "    # Set concurrency\n",
    "    min_concurrency = int(ray.cluster_resources().get(\"CPU\", 1) * 0.4)\n",
    "    max_concurrency = int(ray.cluster_resources().get(\"CPU\", 1))\n",
    "    max_concurrency = int(ray.cluster_resources().get(\"CPU\", 1) * 0.9)\n",
    "\n",
    "    # Process PDFs\n",
    "    pages_dataset = ray_dataset.map_batches(\n",
    "        PDFProcessor,\n",
    "        # PDFProcessor.process_batch,\n",
    "        batch_size=100,\n",
    "        num_cpus=1,\n",
    "        # concurrency=max_concurrency\n",
    "        concurrency=(15, 28),\n",
    "    )\n",
    "\n",
    "    # Perform OCR\n",
    "    # ocr_processor = OCRProcessor()\n",
    "    # ocr_processor = OCRProcessor.remote()\n",
    "\n",
    "    ocr_dataset = pages_dataset.map_batches(\n",
    "        OCRProcessor,\n",
    "        # ocr_processor.process_batch,\n",
    "        batch_size=8,\n",
    "        num_cpus=1,\n",
    "        concurrency=(20, 28),   # recommendation from Ray docs?\n",
    "    )\n",
    "\n",
    "    # Convert to pandas and display results\n",
    "    ocr_dataset_pd = ocr_dataset.to_pandas()\n",
    "    ocr_dataset_pd.display()\n",
    "\n",
    "    processed_spark_df = spark.createDataFrame(ocr_dataset_pd)\n",
    "    processed_spark_df.write.mode(\"overwrite\").saveAsTable(\"alex_m.gen_ai.ray_ocr\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37bd045-4a9a-4abb-847d-d30cf6568d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shutdown_ray_cluster()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-ray-ocr-pipeline-spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
